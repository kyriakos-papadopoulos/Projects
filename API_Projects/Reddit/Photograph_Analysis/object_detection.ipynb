{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dccedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15920428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from skimage import io\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144f329",
   "metadata": {},
   "source": [
    "# Object Identification with YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "weights_path = \"/Users/kyriakospapadopoulos/Desktop/University/Big Blue Data Academy/Personal/Projects/API_Projects/Reddit/Photograph_Analysis/YOLOv3/yolov3.weights\"\n",
    "config_path = \"/Users/kyriakospapadopoulos/Desktop/University/Big Blue Data Academy/Personal/Projects/API_Projects/Reddit/Photograph_Analysis/YOLOv3/yolov3.cfg\"\n",
    "directory = \"/Users/kyriakospapadopoulos/Desktop/University/Big Blue Data Academy/Personal/Projects/API_Projects/Reddit/Photograph_Analysis/photographs_top_1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54339ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model\n",
    "def load_yolo_model(weights_path, config_path):\n",
    "    net = cv2.dnn.readNet(weights_path, config_path)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return net, output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4220d88",
   "metadata": {},
   "source": [
    "### 'load_yolo_model' function breakdown\n",
    "---\n",
    "- **`weights_path`**: This parameter specifies the file path to the YOLO model's pre-trained weights file (e.g., `yolov3.weights`).\n",
    "\n",
    "- **`config_path`**: This parameter specifies the file path to the YOLO model's configuration file (e.g., `yolov3.cfg`). The configuration file contains the model's architecture and other settings.\n",
    "\n",
    "- **`cv2.dnn.readNet(weights_path, config_path)`**:\n",
    "  - This line loads the YOLO model using OpenCV's deep neural network (DNN) module. It takes the pre-trained weights and configuration files as inputs to create a network object (`net`) that can be used for object detection.\n",
    "\n",
    "- **`layer_names = net.getLayerNames()`**:\n",
    "  - This line retrieves the names of all the layers in the YOLO model. The layer names are stored in a list called `layer_names`. These names are necessary to identify which layers are output layers for detection.\n",
    "\n",
    "- **`output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]`**:\n",
    "  - This line identifies the output layers of the YOLO model. The `net.getUnconnectedOutLayers()` function returns the indices of the layers that are not connected to any other layers (i.e., the output layers). The indices are used to extract the corresponding layer names from `layer_names`. The result is stored in the `output_layers` list.\n",
    "\n",
    "- **`return net, output_layers`**:\n",
    "  - The function returns two objects: `net`, which is the loaded YOLO model, and `output_layers`, which is a list of the names of the output layers. These are used in subsequent steps to perform object detection with YOLO.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect objects in an image with Non-Maximum Suppression (NMS) and prevent duplicate assignments\n",
    "def detect_objects_yolo(net, output_layers, image, confidence_threshold=0.5, nms_threshold=0.4):\n",
    "    height, width, channels = image.shape\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    # Load class labels\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    detected_objects = set()  # Use a set to avoid duplicates\n",
    "    \n",
    "    # Loop over each detection\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confidence_threshold:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply Non-Maximum Suppression (NMS)\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            detected_objects.add(classes[class_ids[i]])  # Add to set to avoid duplicates\n",
    "    \n",
    "    return list(detected_objects)  # Convert set back to list for consistency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17cf9f",
   "metadata": {},
   "source": [
    "### 'detect_objects_yolo' function breakdown\n",
    "---\n",
    "- **`height, width, channels = image.shape`**:\n",
    "  - This line extracts the dimensions of the input image (height, width, and the number of color channels).\n",
    "\n",
    "- **`blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)`**:\n",
    "  - The image is preprocessed into a \"blob,\" which is a 4-dimensional binary large object. The blob is scaled by 0.00392 (1/255) to normalize pixel values, resized to 416x416 pixels, and adjusted without cropping. This blob is the input to the YOLO model.\n",
    "\n",
    "- **`net.setInput(blob)`**:\n",
    "  - The blob is fed into the YOLO network as input for object detection.\n",
    "\n",
    "- **`outs = net.forward(output_layers)`**:\n",
    "  - The network performs a forward pass, outputting the results from the specified output layers. These results contain information about detected objects.\n",
    "\n",
    "- **`class_ids`, `confidences`, `boxes`**:\n",
    "  - These lists are initialized to store the class IDs of detected objects, their confidence scores, and the bounding box coordinates, respectively.\n",
    "\n",
    "- **`with open(\"coco.names\", \"r\") as f:`**:\n",
    "  - The class labels (e.g., \"person,\" \"bicycle,\" \"car\") are loaded from the \"coco.names\" file into a list called `classes`.\n",
    "\n",
    "- **`detected_objects = set()`**:\n",
    "  - A set is used to store detected object labels. The use of a set helps avoid duplicate entries.\n",
    "\n",
    "- **Loop over each detection**:\n",
    "  - The outer loop iterates over the detections returned by the model. The inner loop processes each detection, extracting the class ID and confidence score. If the confidence is above the specified threshold (`confidence_threshold`), the bounding box coordinates are calculated and stored in the `boxes` list, while confidence scores and class IDs are stored in `confidences` and `class_ids`.\n",
    "\n",
    "- **Apply Non-Maximum Suppression (NMS)**:\n",
    "  - `cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)` applies Non-Maximum Suppression to filter out overlapping bounding boxes, keeping only the most confident ones. This step helps to reduce duplicate detections of the same object.\n",
    "\n",
    "- **`detected_objects.add(classes[class_ids[i]])`**:\n",
    "  - For each remaining detection after NMS, the class label is added to the `detected_objects` set, ensuring that each object type is only added once.\n",
    "\n",
    "- **`return list(detected_objects)`**:\n",
    "  - The set `detected_objects` is converted back to a list before being returned. This list contains the unique object labels detected in the image.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bf551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all images in the directory and save results incrementally\n",
    "def analyze_with_yolo(directory, weights_path, config_path, save_interval=100):\n",
    "    net, output_layers = load_yolo_model(weights_path, config_path)\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    results_df = pd.DataFrame(columns=['File Name', 'YOLO results'])\n",
    "    \n",
    "    # Loop through all files in the directory with a progress bar\n",
    "    for idx, filename in enumerate(tqdm(os.listdir(directory), desc=\"Running YOLO\")):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):  # Adjust based on your file types\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            if image is None:\n",
    "                print(f\"Failed to load image: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            detected_objects = detect_objects_yolo(net, output_layers, image)\n",
    "            \n",
    "            # Append results to the DataFrame\n",
    "            results_df = results_df.append({\n",
    "                'File Name': filename,\n",
    "                'YOLO results': \", \".join(detected_objects)\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "        # Periodically save progress\n",
    "        if idx % save_interval == 0 and idx != 0:  # Save every `save_interval` images\n",
    "            results_df.to_csv(os.path.join(directory, \"detection_results.csv\"), index=False)\n",
    "    \n",
    "    # Final save\n",
    "    results_df.to_csv(os.path.join(directory, \"detection_results.csv\"), index=False)\n",
    "    print(\"Analysis complete. Results saved to detection_results.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46de74",
   "metadata": {},
   "source": [
    "### 'analyze_with_yolo' function breakdown\n",
    "---\n",
    "- **`net, output_layers = load_yolo_model(weights_path, config_path)`**:\n",
    "  - The YOLO model is loaded using the `load_yolo_model` function. This function returns the network (`net`) and the names of the output layers (`output_layers`), which are necessary for object detection.\n",
    "\n",
    "- **`results_df = pd.DataFrame(columns=['file name', 'objects detected'])`**:\n",
    "  - An empty pandas DataFrame `results_df` is initialized with columns `'file name'` and `'objects detected'`. This DataFrame will store the detection results for each image.\n",
    "\n",
    "- **Loop through all files in the directory**:\n",
    "  - The function iterates over all files in the specified `directory` using a `tqdm` progress bar for visual feedback. Only files with image extensions (e.g., `.jpg`, `.jpeg`, `.png`, `.bmp`) are processed.\n",
    "\n",
    "- **`image_path = os.path.join(directory, filename)`**:\n",
    "  - For each image file, the full path is constructed by joining the directory path with the filename.\n",
    "\n",
    "- **`image = cv2.imread(image_path)`**:\n",
    "  - The image is loaded using OpenCV's `cv2.imread` function. If the image fails to load (e.g., due to corruption), a warning message is printed, and the loop continues to the next image.\n",
    "\n",
    "- **`detected_objects = detect_objects_yolo(net, output_layers, image)`**:\n",
    "  - The function `detect_objects_yolo` is called to detect objects in the image using the YOLO model. This function returns a list of detected object labels.\n",
    "\n",
    "- **Append results to the DataFrame**:\n",
    "  - The detection results, including the filename and a comma-separated string of detected objects, are appended to the DataFrame `results_df`.\n",
    "\n",
    "- **Periodically save progress**:\n",
    "  - Every `save_interval` images, the current state of the DataFrame is saved to a CSV file (`detection_results.csv`) in the specified directory. This ensures that progress is not lost if the process is interrupted.\n",
    "\n",
    "- **Final save**:\n",
    "  - After all images have been processed, the final version of the DataFrame is saved to `detection_results.csv`. A message is printed to confirm that the analysis is complete and the results have been saved.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b625f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_with_yolo(directory, weights_path, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e869e",
   "metadata": {},
   "source": [
    "# Object Identification with Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18387bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = os.path.join(directory, \"detection_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826945a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Faster R-CNN model\n",
    "def load_faster_rcnn_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0faf4a",
   "metadata": {},
   "source": [
    "### 'load_faster_rcnn_model' function breakdown\n",
    "---\n",
    "- **`model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)`**:\n",
    "  - This line loads a pre-trained Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN) architecture. The `pretrained=True` argument specifies that the model should be loaded with weights pre-trained on a dataset like COCO, which allows it to detect common objects without additional training.\n",
    "\n",
    "- **`model.eval()`**:\n",
    "  - This sets the model to evaluation mode using the `eval()` method. In evaluation mode, certain layers like dropout and batch normalization behave differently compared to training mode. This is essential when using the model for inference (i.e., making predictions on new data).\n",
    "\n",
    "- **`return model`**:\n",
    "  - The function returns the loaded and configured Faster R-CNN model, ready to be used for object detection on new images.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf05b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_labels():\n",
    "    coco_labels = {\n",
    "        1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane',\n",
    "        6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light',\n",
    "        11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird',\n",
    "        17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant',\n",
    "        23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella',\n",
    "        31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis',\n",
    "        36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat',\n",
    "        40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket',\n",
    "        44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife',\n",
    "        50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich',\n",
    "        55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza',\n",
    "        60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant',\n",
    "        65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop',\n",
    "        74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave',\n",
    "        79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book',\n",
    "        85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier',\n",
    "        90: 'toothbrush'\n",
    "    }\n",
    "    return coco_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_faster_rcnn(model, image_path, model_labels, confidence_threshold=0.5):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    \n",
    "    labels = predictions[0]['labels'].numpy()\n",
    "    scores = predictions[0]['scores'].numpy()\n",
    "    \n",
    "    detected_objects = set()  # Use a set to avoid duplicates\n",
    "    \n",
    "    for label, score in zip(labels, scores):\n",
    "        if score > confidence_threshold:\n",
    "            detected_objects.add(model_labels.get(label, f\"Unknown label {label}\"))\n",
    "    \n",
    "    return list(detected_objects)  # Convert set back to list for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b549b",
   "metadata": {},
   "source": [
    "### 'detect_objects_faster_rcnn' function breakdown\n",
    "---\n",
    "- **`image = Image.open(image_path).convert(\"RGB\")`**:\n",
    "  - The image is loaded from the specified `image_path` using the PIL library's `Image.open` function. The `convert(\"RGB\")` method ensures that the image is in RGB format, which is required for the model.\n",
    "\n",
    "- **`transform = transforms.Compose([transforms.ToTensor()])`**:\n",
    "  - A transformation pipeline is created using `transforms.Compose`, which converts the image into a PyTorch tensor. The `ToTensor()` transform scales the pixel values to a range of [0, 1] and rearranges the dimensions to match the format expected by the model.\n",
    "\n",
    "- **`image_tensor = transform(image).unsqueeze(0)`**:\n",
    "  - The image is transformed into a tensor and then unsqueezed to add a batch dimension (i.e., changing the shape from `[C, H, W]` to `[1, C, H, W]`), as the model expects a batch of images as input.\n",
    "\n",
    "- **`with torch.no_grad():`**:\n",
    "  - This context manager disables gradient computation, which is unnecessary during inference, reducing memory usage and speeding up the process.\n",
    "\n",
    "- **`predictions = model(image_tensor)`**:\n",
    "  - The Faster R-CNN model is applied to the image tensor to generate predictions. The output is a dictionary containing various prediction results, such as labels, bounding boxes, and confidence scores.\n",
    "\n",
    "- **`labels = predictions[0]['labels'].numpy()`**:\n",
    "  - The predicted class labels for the objects detected in the image are extracted from the model's output and converted to a NumPy array for easier manipulation.\n",
    "\n",
    "- **`scores = predictions[0]['scores'].numpy()`**:\n",
    "  - The confidence scores associated with each detected object are also extracted and converted to a NumPy array.\n",
    "\n",
    "- **`detected_objects = set()`**:\n",
    "  - A set is initialized to store the detected object labels. The use of a set ensures that each object type is only added once, avoiding duplicates.\n",
    "\n",
    "- **Loop through labels and scores**:\n",
    "  - The function iterates over each pair of label and score. If the confidence score exceeds the specified `confidence_threshold`, the corresponding label is added to the `detected_objects` set.\n",
    "\n",
    "- **`detected_objects.add(model_labels.get(label, f\"Unknown label {label}\"))`**:\n",
    "  - For each detected object, the label is looked up in the `model_labels` dictionary to retrieve the human-readable label (e.g., \"cat\", \"dog\"). If the label is not found in the dictionary, a placeholder text like `\"Unknown label {label}\"` is added instead.\n",
    "\n",
    "- **`return list(detected_objects)`**:\n",
    "  - The set of detected objects is converted back to a list for consistency before being returned by the function. This list contains the unique object labels detected in the image.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_faster_rcnn(directory, model, df_path, save_interval=100):\n",
    "    # Load the existing DataFrame with YOLO results\n",
    "    if os.path.exists(df_path):\n",
    "        df = pd.read_csv(df_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The specified DataFrame file does not exist: {df_path}\")\n",
    "\n",
    "    # Load COCO labels once\n",
    "    model_labels = load_coco_labels()\n",
    "\n",
    "    # Add a new column for Faster R-CNN results if it doesn't exist\n",
    "    if 'Faster R-CNN results' not in df.columns:\n",
    "        df['Faster R-CNN results'] = \"\"\n",
    "\n",
    "    # Process each image in the DataFrame\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Running Faster R-CNN\"):\n",
    "        image_path = os.path.join(directory, row['File Name'])\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            detected_objects = detect_objects_faster_rcnn(model, image_path, model_labels)\n",
    "            df.at[idx, 'Faster R-CNN results'] = \", \".join(detected_objects)\n",
    "        \n",
    "        if idx % save_interval == 0 and idx != 0:\n",
    "            # Save progress periodically\n",
    "            df.to_csv(df_path, index=False)\n",
    "    \n",
    "    # Save the final updated DataFrame\n",
    "    df.to_csv(df_path, index=False)\n",
    "    print(f\"Faster R-CNN analysis complete. Results saved to {df_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c913d",
   "metadata": {},
   "source": [
    "### 'analyze_with_faster_rcnn'  function breakdown\n",
    "---\n",
    "- **`if os.path.exists(df_path): df = pd.read_csv(df_path)`**:\n",
    "  - The function checks if the specified DataFrame file (`df_path`) exists. If it does, the DataFrame is loaded using `pd.read_csv(df_path)`, which contains results from previous YOLO analysis. If the file does not exist, a `FileNotFoundError` is raised.\n",
    "\n",
    "- **`model_labels = load_coco_labels()`**:\n",
    "  - The COCO labels (i.e., human-readable class names) are loaded once using the `load_coco_labels()` function. These labels are used to map the model's predicted class IDs to descriptive names.\n",
    "\n",
    "- **`if 'Faster R-CNN results' not in df.columns: df['Faster R-CNN results'] = \"\"`**:\n",
    "  - The function checks if the DataFrame already contains a column for storing Faster R-CNN results. If the column does not exist, it is added and initialized with empty strings for all rows.\n",
    "\n",
    "- **`for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Running Faster R-CNN\"):`**:\n",
    "  - The function iterates over each row in the DataFrame using `iterrows()` and `tqdm` to display a progress bar. Each row corresponds to an image file previously analyzed by YOLO.\n",
    "\n",
    "- **`image_path = os.path.join(directory, row['file name'])`**:\n",
    "  - For each row in the DataFrame, the full path to the corresponding image is constructed by joining the `directory` path with the filename stored in the `'file name'` column.\n",
    "\n",
    "- **`if os.path.exists(image_path): detected_objects = detect_objects_faster_rcnn(model, image_path, model_labels)`**:\n",
    "  - If the image file exists at the specified path, the function calls `detect_objects_faster_rcnn()` to perform object detection using the Faster R-CNN model. The detected objects are returned as a list.\n",
    "\n",
    "- **`df.at[idx, 'Faster R-CNN results'] = \", \".join(detected_objects)`**:\n",
    "  - The detected objects are stored in the `'Faster R-CNN results'` column of the DataFrame for the corresponding row. The detected object labels are joined into a comma-separated string.\n",
    "\n",
    "- **`if idx % save_interval == 0 and idx != 0: df.to_csv(df_path, index=False)`**:\n",
    "  - Every `save_interval` iterations, the function saves the current state of the DataFrame to the specified CSV file (`df_path`). This ensures that progress is saved periodically, reducing the risk of data loss in case of an interruption.\n",
    "\n",
    "- **`df.to_csv(df_path, index=False)`**:\n",
    "  - After all images have been processed, the final version of the updated DataFrame is saved to the specified CSV file. A message is printed to confirm that the Faster R-CNN analysis is complete and the results have been saved.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Faster R-CNN model and labels\n",
    "model_labels = load_coco_labels()\n",
    "model = load_faster_rcnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2101e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze_with_faster_rcnn(directory=directory, model=model, df_path=df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b1420",
   "metadata": {},
   "source": [
    "---\n",
    "## Models for future consideration.\n",
    "\n",
    "### 1. **OpenAI's CLIP**\n",
    "   - **Description**: CLIP can understand and classify images based on natural language prompts. It's highly versatile and can recognize a wide range of objects and scenes.\n",
    "   - **Use Case**: It's more versatile and can match images to descriptive text prompts like \"a city skyline,\" \"a mountain landscape,\" or \"a person at the beach\" ; providing more information can be useful if NLP analysis is performed on the results.\n",
    "   - **Link**: [CLIP on GitHub](https://github.com/openai/CLIP)\n",
    "\n",
    "### 2. **Google Vision API (Free Tier)**\n",
    "   - **Description**: While primarily a cloud service, Google Vision API offers a free tier with limited usage. It can identify a vast array of objects and scenes without any fine-tuning required.\n",
    "   - **Use Case**: Upload images to the API and receive labels such as \"city,\" \"tree,\" \"building,\" \"sea,\" and more. Can generate many more labels than the models I am using righn now.\n",
    "   - **Link**: [Google Cloud Vision API](https://cloud.google.com/vision)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedefd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915aa312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
